
\documentclass[journal]{IEEEtran}

\usepackage{bbm}
\newcommand{\field}[1]{\mathbb{#1}}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{cite}
\usepackage{bm}

\usepackage{dsfont}
\usepackage{yonatan}
\def\I{\ensuremath{\mathds{1}}}
\DeclareMathOperator*{\argmin}{argmin}

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{../figures/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % \DeclareGraphicsExtensions{.eps}
\fi
\usepackage{array}
%\usepackage[tight,footnotesize]{subfigure}
\usepackage{subcaption}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{color}
%\usepackage{cleveref}
\usepackage{hyperref}



\begin{document}
%
\title{Literature Review}
\author{Yonatan~Vaizman}

\markboth{}%
{Vaizman \MakeLowercase{\textit{et al.}}: }

\maketitle



\section{Audio processing}

\cite{spanias1994speechCoding} for speech coding tutorial.....

\cite{kameoka2010speech} for joint spectral-envelope and f0 estimation....

\cite{khanagha2013efficient} efficient solution to sparse linear prediction analysis for speech...

Books: \cite{rabiner1978digital,chu2004speechCoding,kleijn1995speech,Rabiner93}

\subsection{LPC}
Good tutorial for linear prediction framework \cite{makhoul1975linearPredictionTutorial}.
\cite{makhoul1977stableLattice,makhoul1985VQ}...
\cite{giacobello2008sparse,giacobello2012sparse} sparse linear prediction for speech....
\subsection{Vocoders}
\cite{makhoul1976vocoderSpeechQuality}....
\subsection{Frequency-warped LPC}
\subsection{Fourier space excitation-filter framework}
\subsection{Filterbanks (overcomplete codebooks?) for speech analysis}
Speech coding with VQ \cite{makhoul1985VQ}....

\section{Optimization and learning}
\subsection{LPC optimization}
Good tutorial for linear prediction framework \cite{makhoul1975linearPredictionTutorial}. Makhoul talks about the squared error criterion, its advantages and shortcomings and when it might not fit the best spectral envelop (when the excitation input to the system is a pitched periodic pulse train --- in which case the peaks of the estimated filter coincide with the harmonics instead of the better fitting envelop).

In \cite{el1991discrete} El-Jaroudi \etal give intuition to the inappropriateness of traditional linear prediction with L2 minimization. They discuss the case of signals that have discrete spectra, such as voiced speech, which theoretically only has power in the fundamental frequency and in its harmonic multiples. They regard to more general cases, where there is only power in a discrete, finite set of frequencies (not necessarily integer multiples of a fundamental). According to the excitation-filter model such observed signals were generated when the resonating all-pole filter was excited with a discrete-spectrum excitation signal. Since the power of the system is only sampled at discrite points, the autocorrelation of the observed signal has strong aliasing and repetitions. This aliasing is more severe when the frequency-sampling is more sever (when the fundamental is a higher frequency). Because of this aliased version of the original system's autocorrelation (inverse transform of the filter's power spectrum), the unique solution of filter coefficients that the LP finds is not the original (``true'') filter that generated the signal. In their work the authors suggested Discrete All-Pole (DAP) modeling, in which an autocorrelation signal is calculated only using the specific frequencies that have effective power (as a discrete inverse transform --- sum over the finite set of ``on'' frequencies, instead of integral over all frequencies). They optimize the Itakura-Saito error measure in its discrete-frequency version, and suggest an iterative algorithm to solve for the optimal filter.

However, the usage of their algorithm requires to first estimate the power spectrum of the observed signal and perform peak picking to locate the discrete (and finite) set of frequencies to be used in the algorithm. In practice, when noisy observations are present, the stage of peak picking depends on other algorithms and may introduce biases and errors.

\subsection{L2, L1, Lp minimizations}
In \cite{denoel1985leastAbsolute} Denoel and Solvay analyze using linear prediction with L1 minimization (least absolute error criterion) for speech. This optimization problem is still convex, but usage of Linear Programming to solve it requires heavy computation and doesn't guarantee that the selected optimal filter is stable. The authors use a lattice structure and derive a Burg-like order-recursive algorithm to optimize the reflection coefficients one after the other. This suggested algorithm does guarantee a stable filter for every order. The heavy computation of this algorithm lies in a median calculation for every order. When the application is coding for transmission, this can be alleviated by replacing sorting of values with bisection to bins (according to the number of bits provided for the coding of a reflection coefficient), and the leftover error can be compensated in the next order. However, if quantization and coding are not done and simply the optimal filter is estimated, this method is still computationally heavier than L2 methods.

Linear prediction with L1 norm \cite{schroeder1989linear}...

Adaptive Lp linear prediction \cite{lansford1988adaptive}...

\cite{giacobello2008sparse,giacobello2012sparse} sparse linear prediction for speech....

Stable IIR design based on Lp error minimization \cite{tseng2004design}....

Giri and Rao - block sparse excitation criterion \cite{giri2014block}...
\subsection{Optimization of mixtures}
\subsection{Sparsecoding and compressed sensing}
\subsection{LTI filter clustering}
Perceptually consistent measures of spectral distance \cite{viswanathan1976spectralDistance}....

Speech coding with VQ \cite{makhoul1985VQ}....

Spectral distance measures \cite{viswanathan1976spectralDistance}....

VQ in speech coding \cite{gersho1992vq}...

\section{MIR applications}
\subsection{Source separation}
\cite{slaney1994soundSeparation}....

\cite{ozerov2012general} ....

\subsection{Melody extraction and automatic transcription}

\cite{benetos2013automatic,ohanlon2013automatic,peeters2006music}....
Guitar chords and fingering \cite{barbancho2012automatic}....
\subsection{Chord recognition}
\subsection{Instrument recognition}
Classification of pitched musical instruments \cite{herrera2006automatic}...

\cite{martin1998musical,yu2014sparse}....

Polyphonic and polyinstrument \cite{hamel2009automatic}....

Instrument recognition (temporal and cepstral features) \cite{eronen2000musical}.... and comparison of acoustic features \cite{eronen2001comparison}....

Identifying woodwind instruments \cite{brown2001feature}.....

instrument recognition and affect on mir \cite{kitahara2007computational}....

temporal integration \cite{Joder:09}....

Multitrack mixing: \cite{scottinstrument,scott2011automatic}....

In \cite{tjoa2010musical} isolated sound single instrument recognition was performed using recordings from UIowa, McGill, OLPC and Freesound datasets, with total of 24 instruments.
For each separate note (or beat) first STFT was calculated, then non-negative matrix factorization (NMF) was performed on the magnitude STFT. Each atom of the approximation factorization (each combination of spectral column and temporal row) was analyzed separately in the spectral component (the column vector was processed to produce 32 MFCCs) or in the temporal component (the row vector was processed with a predetermined 32-channel Gamma filterbank, and from each response signal the maximal value was retained, for time-shift invariance). RBF-kernel SVM classification was done either using only spectral features (32 MFCCs of an atom), only temporal features (32 Gamma response maxima of an atom), or concatenation of both (64 dimensions per atom). Each atom was regarded as an instance and classified.
In their results they observed generally better performance by spectral features compared to temporal features, but significant improvement when using both types of features.

In \cite{fuhrmann2011quantifying} Fuhrmann \etal compared different methods to select segments out of a musical excerpt (full production), and apply predominant instrument recognition only on those selected segments. The methods were: taking all the excerpt (regarded as ``upper bound''), taking the first 30 seconds, taking uniformly spaced segments and clustering segments. They shown that selecting with clustering is better than the other segment-selection methods. The whole point of using selected segments was efficiency: to analyse less audio time (taking advantage of the redundancy and repetitiveness common in songs). However, in order to ``cleverly'' select the segments additional analysis should be done (the clustering), using features from all the excerpt, and the selected segments in the clustering methods occupied, in average, 0.66 of the audio time, which is still a lot of time to analyze. In addition, the experiments were done only on 220 excerpts of 30 seconds each, and the results don't show much of an improvement compared to the runtime-cheaper segment-selection methods.


In \cite{bosch2012comparison} Bosch \etal suggested combining a pre-stage of an of-the-shelf source separation algorithm (FASST - Flexible Audio Source Separation Framework by Ozerov \etal \cite{ozerov2012general}) before performing instrument recognition. They applied standard separation to 4 channels: bass, drums, melody and other. They observed that when using the same models for instrument recognition on each channel and then combining, the results are worse than using the original audio. However, when training SVM models separately on each of the 4 channels, the result is an improvement.

\section{Datasets}

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{IEEEtran}
\bibliography{References}



% that's all folks
\end{document}


